---
description: Mon 20th Oct - Sun 26th Oct 2025
---

# Week 43

## Thursday 23rd October 2025


### AI Ethics WG

- **Type of meeting:** One-off event
- **Present:** LadyTempestt [**facilitator**], lola lawson, Stephen [QADAO] [**documenter**], LadyTempestt, Sucre n Spice, CallyFromAuron, EstherG, Kare Dahl, Mariia Lagutina, Prosper Etim, AshleyDawn, Alfred Itodele, Ayomi Shuga, PeterE, Sharmila, CollyPride, UKnowZork, lola lawson, others
- **Purpose:** Session 1 of the BGI-25 Virtual Unconference: "Interviewing the public on AI ethics"
- **Meeting video:** [Link](https://youtu.be/PN5kYh4YZTo?si=awcEOs86fIV4KwZg)
- **Working Docs:**

#### Narrative:
PRESENTATIONS
The sNET Ambassador Program’s AI Ethics workgroup used interviews (recorded, anonymised, and coded) to explore public opinion on AI ethics.

Our interview process: 
- Using interviewers from the same demographic group as the interviewees elicited deeper insights; 
- Language access: Interviewing in languages other than English is a long-term aim, to avoid limiting whose voices are heard; sometimes, interviews were stilted if both interviewer and interviewee were not first-language English speakers. But a multi-lingual interview corpus will demand additional overhead for translation and checking the accuracy of translations 
- Beginner interviewers need some training, but “learning by doing” works well too
- Transcription is important (to allow for varying audio quality, and for different accents)
- Interviewers transcribing their own interviews was initially the plan, but in practice it didn’t necessarily produce better transcripts
- AI transcription tools save time, and helpwith poor audio quality (due to e.g. bad Internet connections), but often misinterpret words, accents and background noise; so they were only used as a first pass, and human editing was always needed. 

Early analysis and coding has found three key themes in the interviews: 
- human oversight is crucial for safety. AI autonomy without human control was seen as risky and disruptive.
- inclusivity is itself a form of safety - people of different cultures and languages should be part of building and controlling AI, not just using it. People trust what they help to build
- AI is a tool, not a replacement for people - but some do use it as a companion.


DISCUSSION
1. Governance, trust, and power
 - Government vs. corporate trust: Some participants in the session, especially those in Nigeria, expressed low trust in the idea of governments controlling the development of AI due to issues such as censorship, manipulation of narratives, and low digital awareness amongst politicians. Some participants were more open to corporate involvement, but still worried about privacy and data misuse; they said it would be better to have multiple individuals hold the power in AI design and control, and preferred the idea of community-driven, participatory governance instead of putting power in the hands of one entitiy, such as a government or a private company.

- Policy suggestions: Regulations should fit local contexts, include community voices, and avoid repeating “top-down” approaches. Collaborative and locally-led governance of AI was encouraged.


2. Bias, datasets, and justice
- Technical and social bias (e.g. in image generation tools): the need for both better dataset design and clearer legal protections. 

- Bias examples: Real examples were discussed, such as image-generation tools that have shown black people negatively and other races positively, indicating how social bias can be embedded in AI systems as a result of the biases of the system creators.

- Issues such as recruiters using AI to determine if someone’s CV is AI generated - can we trust these kinds of uses of AI?

- Legal gaps: Existing laws don’t fully address biased data training or misuse. Stronger accountability and rights around data ownership and retraining were recommended.

- True inclusion: Inclusion isn’t just about adding people from different backgrounds; it’s also about including their values, experiences, and cultural knowledge in AI design.


3. AI design, bias, and sexism
- Many AI assistants use female voices and have female names, which can  reinforce sex  stereotypes of service roles being female-coded. The group suggested chatbots with a wider range of gendered voices, or gender-neutral voices, to undermine these stereotypes.

4. Data privacy and consent
- Participants raised concerns about unclear privacy policies in AI tools, lack of clarity of what personal data will be used for (e.g. whether it will be used to train AI further), and the difficulty of deleting personal data once it’s online. Consent should be more transparent and easy to manage.
- Data sovereignIty: Data collected from local communities should remain beneficial to those communities and not be exploited by centralised organisations.


TAKEAWAYS/LEARNING POINTS:
- Use AI transcription tools to save time, but always follow up with a human review before using the data.
- Recruit more  marginalized voices as interviewers to make research fairer, more accurate and more inclusive.
- Create community-led policy drafts that protect data ownership and promote accountability.
- Explore customizable voice options to remove gender and cultural bias.
- When designing AI systems, human oversight, cultural inclusivity,  and including human empathy are just as important as technical performance.
- How we transcribe and document qualitative research affects how findings are understood.
- AI governance should be community-based, not imposed from outside.
- Tackling bias requires work at the dataset and design levels, with laws to back it up.
- Next steps include training a wide range of interviewers, and building community-informed governance for African contexts.



#### Keywords/tags:
- **topics covered:** AI ethics, interviewing, qualitative research, transcription, AI bias, data soveregnity, human-in-the-loop, gender bias, sex bias, sexism, African AI, Nigeria, AI governance, dataset fairness, recruitment, privacy, Anonymity & Transparency, Consent, data privacy, chatbots, AI design
- **emotions:** reflective, honest, culturally conscious,  insightful, grounded, urgent,  Collaborative, lively

### AI Sandbox/Think-tank

- **Type of meeting:** Think-Tank
- **Present:** osmium [**facilitator**], Stephen [QADAO] [**documenter**], osmium, LordKizzy, others unknown
- **Purpose:** Session 2 of the BGI-25 Virtual Unconference: "AI Agent DAO Blueprint"
- **Media link:** [Link](https://docs.google.com/presentation/d/1SUT5hd24HLzbxHqT88pkPybP-Ov_YhAii7qL3mARdeA/edit?usp=sharing)
- **Working Docs:**
  - [Slides: AI Agent DAO Blueprint](https://docs.google.com/presentation/d/1SUT5hd24HLzbxHqT88pkPybP-Ov_YhAii7qL3mARdeA/edit?usp=sharing)
  - [Blog post - From Idea to Action: my experience showcasing the AI Agent DAO at BGI-25](https://docs.google.com/document/d/1lufZVGjK4Ltm-QmGd34iMcAxYh-LCj80oSEBJNgaBDc/edit?usp=sharing)

#### In this meeting we discussed:
- Vasu Madaan presented his idea for a new approach to regulating AI agents on tthe SingularityNET tech stack, by means of a DAO structure.

This would allow a decentralised approach to managing ethical risks, via a common ethical framework to guide the behaviour of AI agents, and a reputation system to evaluate and enhance their rustworthiness and accountability.

Key system components:
- superintelligence.io: ASI coordination
- singularitynet.io: AI services
- fetch.ai: Autonomous infrastructure
- agentverse.ai: Deployment layer
- hyperon.opencog.org: Cognitive core

See 
- slide deck here https://docs.google.com/presentation/d/1SUT5hd24HLzbxHqT88pkPybP-Ov_YhAii7qL3mARdeA/edit?usp=sharing 
- blogpost here https://docs.google.com/document/d/1lufZVGjK4Ltm-QmGd34iMcAxYh-LCj80oSEBJNgaBDc/edit?usp=sharing

for more details


#### Keywords/tags:
- **topics covered:** DAOs, AI Agent, Decentralization, AI ethics, AI safety, accountability, human-in-the-loop, Human values, Fetch Ai, agentverse, OpenCOG Hyperon